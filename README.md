# AI vs Human 文章偵測器（AI Detector）

## 📋 專案概述

這是一個簡單而強大的 **AI vs Human 文章分類工具**，可以幫助用戶快速判斷一段文本是否由人工智能生成。該工具基於教學文檔中的高階理論（Perplexity、Burstiness、Stylometry 等），並使用 Streamlit 提供友好的用戶界面，支援中英文混合文本分析。


---

## 🎯 功能特性

### 核心功能
- ✅ **即時文本分析**：輸入文本立即顯示 AI% / Human% 判定結果
- ✅ **多維度特徵分析**：包含 **17 個精心設計的統計特徵**
- ✅ **可視化展示**：提供概率分佈、句長分析、特徵值對比等視覺化圖表
- ✅ **詳細統計量**：展示每個特徵的具體數值，便於深入理解文本特性
- ✅ **雙模型集成**：結合 Logistic Regression 和 Random Forest 提高準確度
- ✅ **自適應界面**：響應式設計支持多種設備顯示

### 核心特徵（基於教學理論）

#### 1. **句子節奏（Burstiness）** - 最重要的特徵
```
Burstiness = σ(sentence_length) / μ(sentence_length)
```
- **AI 文本**：通常具有較低的 Burstiness（句子長度穩定，~0.25）
- **Human 文本**：通常具有較高的 Burstiness（長短句交錯，~0.6）
- **解釋**：人類寫作會根據表達需要變換句長，而 AI 傾向生成均勻長度的句子

#### 2. **詞彙多樣性（TTR - Type-Token Ratio）**
```
TTR = 不重複詞彙數 / 總詞彙數
```
- **計算方式**：統計詞彙豐富度指標
- **AI 特徵**：TTR 通常較低（~0.55），傾向重複使用常見詞彙
- **Human 特徵**：TTR 通常較高（~0.70），詞彙選擇更多樣
- **應用**：高 TTR 通常指示人類撰寫

#### 3. **Stylometry（文風統計）** - 多層次特徵
##### Lexical (詞彙層)
- 功能詞比例（的、了、是、在...）
- 詞長分布及平均詞長
- 詞頻分布的Gini係數

##### Syntactic (句法層)
- 標點符號使用比例
- 句式重複度（重複結構出現頻率）
- 從句出現頻率

##### Emotion & Noise (情感與噪音)
- 常見連接詞比例（因此、另外、同時、總之）
- 問號/驚嘆號使用頻率
- 自然語言中的「嗯」、「呃」等填充詞

#### 4. **Zipf's Law（長尾分布）**
```
詞頻排名 × 詞頻 ≈ 常數
```
- **計算**：罕見詞（出現 1 次）的比例
- **AI 特徵**：長尾詞比例較低（~0.35），更傾向使用常見詞
- **Human 特徵**：長尾詞比例較高（~0.55），包含更多獨特詞彙
- **含義**：反映詞彙使用的創意性和多樣性

#### 5. **詞頻熵（Entropy）** - 信息論角度
```
H = -Σ(p_i * log₂(p_i))，其中 p_i 是詞 i 的概率
```
- **AI 文本**：熵值較低（~3.2），詞彙使用較有規律
- **Human 文本**：熵值較高（~4.5），詞彙使用更隨意
- **物理意義**：衡量詞彙使用的不可預測性

#### 其他特徵
| 特徵名稱 | AI 值 | Human 值 | 解釋 |
|---------|------|---------|------|
| 被動語態指標 | ~0.10 | ~0.04 | AI 更常使用被動語態 |
| 常見連接詞比例 | ~0.50 | ~0.20 | AI 更依賴邏輯連接詞 |
| 問號比例 | ~0.05 | ~0.15 | Human 提問更頻繁 |
| 逗號比例 | ~0.70 | ~0.40 | AI 標點使用更規則 |

---

## 🚀 快速開始

### 環境需求
- **Python**：3.8 或更高版本
- **操作系統**：Windows / macOS / Linux
- **網絡**：需要網絡連接以運行 Streamlit

### 安裝步驟

#### 方法 1：使用 Batch 腳本（Windows）- 最簡單 ⭐
```bash
# 進入專案目錄
cd c:\Users\YULUN\Desktop\AIOT\AIOT_HW5

# 運行批處理文件
run.bat
```

批處理腳本將自動：
1. 檢查 Python 環境
2. 安裝依賴包
3. 啟動 Streamlit 應用

#### 方法 2：手動安裝（所有平台）

**步驟 1：進入專案目錄**
```bash
cd /path/to/AIOT_HW5
```

**步驟 2：創建虛擬環境（可選但推薦）**
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS / Linux
python3 -m venv venv
source venv/bin/activate
```

**步驟 3：安裝依賴套件**
```bash
pip install -r requirements.txt
```

**步驟 4：啟動應用**
```bash
streamlit run ai_detector.py
```

應用將在瀏覽器中自動打開（通常為 **http://localhost:8501**）

#### 方法 3：Docker 部署（可選）
```bash
# 構建 Docker 鏡像
docker build -t ai-detector .

# 運行容器
docker run -p 8501:8501 ai-detector
```

---

## 📖 使用方法

### 基本操作

#### 1️⃣ **輸入文本**
- 在文本區域粘貼或輸入要分析的內容
- 最少需要 **50 個字符**（約 20-30 個詞）
- 支持中英文混合、標點符號自動識別

#### 2️⃣ **點擊分析**
- 按下「🔍 **立即分析**」按鈕
- 系統將自動提取 17 個特徵並計算概率

#### 3️⃣ **查看結果**
- 🤖 **AI 概率**：文本為 AI 生成的概率（0-100%）
- 👤 **Human 概率**：文本為人工撰寫的概率（0-100%）
- 總和始終為 100%

### 側邊欄選項

#### ☑️ **顯示詳細特徵**
展示所有 17 個特徵的具體數值，包括：
- 句子節奏特徵（Mean、StdDev、Burstiness）
- 詞彙特徵（TTR、平均詞長、熵）
- 文風特徵（標點、連接詞、被動語態）

#### ☑️ **顯示可視化圖表**
生成兩個互動式圖表：
- **句長分佈直方圖**：展示句子長度的分佈情況
- **高頻詞彙排行**：顯示 Top 10 最常用的詞彙

---

## 📊 輸出解讀指南

### 判定結論解讀表

| AI 概率範圍 | 判定 | 置信度 | 說明 |
|-----------|------|--------|------|
| > 80% | 🚨 **高度確定為 AI 生成** | 很高 | 具有明顯的 AI 文本特徵，多個指標指向 AI |
| 70-80% | ⚠️ **很可能為 AI 生成** | 高 | 主要特徵與 AI 相符，建議進一步驗證 |
| 60-70% | ⚡ **傾向 AI 生成** | 中 | 有一定的 AI 跡象，但也可能是風格化人文 |
| 40-60% | 🔄 **混合特徵** | 低 | 難以判斷，可能是編輯過的 AI 或潤色的人文 |
| 30-40% | 🔙 **傾向人類撰寫** | 中 | 有一定的人類跡象，但也可能是簡化的 AI |
| 20-30% | ✅ **很可能為人類撰寫** | 高 | 主要特徵與人類相符 |
| < 20% | ✨ **高度確定為人類撰寫** | 很高 | 具有明顯的人文特徵 |

### 特徵解讀 - 典型對比

#### 🤖 AI 文本的典型特徵：
```
✓ 低 Burstiness (0.20-0.35)
  → 句子長度穩定均勻
  
✓ 較低的詞彙多樣性 (TTR: 0.50-0.60)
  → 傾向重複使用常見詞彙
  
✓ 較高的常見連接詞比例 (0.40-0.60)
  → 「因此」「另外」「同時」使用頻繁
  
✓ 較低的 Zipf 長尾詞比例 (0.30-0.40)
  → 罕見詞使用較少
  
✓ 較規則的標點使用
  → 標點符號位置可預測
  
✓ 較多被動語態 (0.08-0.12)
  → 「被...」「被...」結構多見
```

#### 👤 Human 文本的典型特徵：
```
✓ 高 Burstiness (0.50-0.80)
  → 長短句交錯，適應表達需要
  
✓ 較高的詞彙多樣性 (TTR: 0.65-0.80)
  → 詞彙選擇豐富多樣
  
✓ 較低的常見連接詞比例 (0.10-0.30)
  → 邏輯詞使用自然
  
✓ 較高的 Zipf 長尾詞比例 (0.50-0.65)
  → 包含更多獨特、創意詞彙
  
✓ 自然的標點使用
  → 標點位置因意思而定
  
✓ 存在自然噪音
  → 錯字、感嘆詞、口語成分
  
✓ 較少被動語態 (0.02-0.06)
  → 主動表達為主
```

---

## 🔬 技術架構

### 系統架構圖
```
┌─────────────────────────────────────────────────┐
│           使用者輸入 (文本)                       │
└────────────────────┬────────────────────────────┘
                     ↓
┌─────────────────────────────────────────────────┐
│      特徵提取層 (Feature Extraction)              │
│  ├─ 句子分割 (Sentence Segmentation)             │
│  ├─ 分詞 (Tokenization)                         │
│  └─ 17維特徵計算                                │
└────────────────────┬────────────────────────────┘
                     ↓
┌─────────────────────────────────────────────────┐
│      特徵標準化層 (Normalization)                 │
│  └─ StandardScaler 正規化到 μ=0, σ=1            │
└────────────────────┬────────────────────────────┘
                     ↓
┌─────────────────────────────────────────────────┐
│      分類層 (Classification)                     │
│  ├─ 邏輯迴歸 (Logistic Regression)              │
│  ├─ 隨機森林 (Random Forest)                    │
│  └─ 集成結果 (Ensemble)                         │
└────────────────────┬────────────────────────────┘
                     ↓
┌─────────────────────────────────────────────────┐
│      輸出層 (Output)                             │
│  ├─ AI 概率 (0-1)                              │
│  ├─ Human 概率 (0-1)                           │
│  └─ 可視化展示                                  │
└─────────────────────────────────────────────────┘
```

### 使用的技術棧

| 模組 | 庫 | 用途 |
|------|-----|------|
| **自然語言處理** | NLTK | 分詞、斷句 |
| **數據處理** | NumPy、Pandas | 特徵計算、數據整理 |
| **機器學習** | scikit-learn | LogisticRegression、RandomForest |
| **前端框架** | Streamlit | Web UI 和互動界面 |
| **數據可視化** | Matplotlib、Seaborn | 圖表繪製 |
| **統計計算** | Collections、scipy | 詞頻統計、熵計算 |

### 核心算法

#### 特徵提取流程
```python
1. 文本預處理
   ├─ 去除空白
   ├─ 分割句子
   └─ 分詞

2. 統計計算
   ├─ 句長統計 (μ, σ)
   ├─ 詞頻分析 (Counter)
   ├─ 熵值計算 (Shannon Entropy)
   └─ 比例計算 (各類特徵)

3. 特徵標準化
   └─ StandardScaler: (x - μ) / σ

4. 概率預測
   ├─ LogisticRegression.predict_proba()
   ├─ RandomForest.predict_proba()
   └─ 平均結果
```

---

## ⚠️ 重要限制和注意事項

### 1. **非決定性工具** ⚡
- ✋ **本工具提供的是「訊號與機率」，而非「絕對判定」**
- 不應作為學術倫理調查或法律訴訟的唯一證據
- 建議結合多種方法進行綜合判斷
- **當 AI 概率在 40-60% 時，置信度最低，需人工審查**

### 2. **語言限制** 🌐
- 主要針對**中文優化**，英文效果會降低
- 可能不適用於其他語言（日語、韓語等）
- 對於極短文本（< 50 詞）準確性較低
- 建議文本長度 **200-500 詞** 以上以獲得最佳效果

### 3. **對抗性文本** 🛡️
- **AI 文本經過人工修改後**可能被誤判為人類文本
- **AI 故意注入錯字和口語**可能規避偵測
- 專業潤稿編輯可能改變文本特徵
- 這是特徵提取方法的內在限制

### 4. **Domain 差異** 📚
| 文本類型 | 適用性 | 說明 |
|---------|--------|------|
| 新聞報道 | ⭐⭐⭐ | 效果最好 |
| 學術論文 | ⭐⭐ | 風格相似度高 |
| 社交媒體 | ⭐⭐ | 特徵差異大 |
| 程式碼註釋 | ⭐ | 不推薦 |
| 詩歌/文學 | ⭐ | 風格差異過大 |

- 不同文本域（Domain）的 AI 和人類文本特徵差異很大
- 可能需要針對特定域重新訓練模型

### 5. **模型局限** 🤖
- 基於**合成訓練數據**（非真實大規模語料）
- 模型複雜度有限（17 維特徵）
- 對**新型 AI 模型**（如 GPT-4 最新版本）的偵測效果未知
- **漸進式對抗**：隨著 AI 改進，偵測會變得更困難

### 6. **數據隱私** 🔒
- 文本不會被發送到遠程服務器（本地運行）
- 使用後結果不會被保存（刷新頁面即清空）

---

## 🛠️ 進階使用和自訂

### 修改特徵權重和訓練數據

編輯 [`ai_detector.py`](ai_detector.py) 中的 `AIDetectorModel.train_sample_model()` 方法：

````python
def train_sample_model(self):
    """訓練模型"""
    # AI 特徵分佈（調整這些參數以適應不同場景）
    ai_features = [
        np.random.normal(15, 3),      # sentence_length_mean: μ=15, σ=3
        np.random.normal(4, 1),       # sentence_length_std
        np.random.normal(0.25, 0.05), # burstiness (AI 特徵：低值)
        np.random.normal(0.55, 0.05), # type_token_ratio
        # ... 調整其他特徵 ...
    ]
    
    # Human 特徵分佈
    human_features = [
        np.random.normal(12, 5),      # sentence_length_mean: 波動更大
        np.random.normal(8, 2),       # sentence_length_std
        np.random.normal(0.6, 0.15),  # burstiness (Human 特徵：高值)
        np.random.normal(0.7, 0.1),   # type_token_ratio
        # ... 調整其他特徵 ...
    ]
