import streamlit as st
import numpy as np
import pandas as pd
import re
import os
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

font_path = os.path.join(os.path.dirname(__file__), "fonts", "NotoSansTC-VariableFont_wght.ttf")
font_prop = fm.FontProperties(fname=font_path)

plt.rcParams['font.family'] = font_prop.get_name()
plt.rcParams['axes.unicode_minus'] = False


import seaborn as sns
from collections import Counter
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from typing import Tuple, Dict

# ============================================================
# é¡µé¢é…ç½®ä¸æ ·å¼
# ============================================================

st.set_page_config(
    page_title="AI æ–‡ç« åµæ¸¬å™¨",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰ CSS æ ·å¼
def load_custom_css():
    st.markdown("""
    <style>
    /* ä¸»å®¹å™¨ */
    .main {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        padding: 20px;
    }
    
    /* æ ‡é¢˜æ ·å¼ */
    .main-title {
        text-align: center;
        color: #1e3c72;
        font-size: 3.5rem;
        font-weight: 900;
        margin-bottom: 0.5rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    
    .subtitle {
        text-align: center;
        color: #555;
        font-size: 1.2rem;
        margin-bottom: 2rem;
        font-weight: 300;
    }
    
    /* ç»“æœå¡ç‰‡ */
    .result-card {
        border-radius: 15px;
        padding: 25px;
        margin: 15px 0;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        border: none;
        text-align: center;
        transition: transform 0.3s ease;
    }
    
    .result-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 12px 40px rgba(0,0,0,0.15);
    }
    
    .card-ai {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
    }
    
    .card-human {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        color: white;
    }
    
    .card-title {
        font-size: 1rem;
        font-weight: 600;
        margin-bottom: 10px;
        opacity: 0.9;
    }
    
    .card-value {
        font-size: 3rem;
        font-weight: 900;
        margin: 10px 0;
    }
    
    .card-label {
        font-size: 0.85rem;
        opacity: 0.8;
        margin-top: 5px;
    }
    
    /* è¾“å…¥åŒºåŸŸ */
    .input-section {
        background: white;
        border-radius: 12px;
        padding: 20px;
        margin: 20px 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
    }
    
    .section-title {
        color: #1e3c72;
        font-size: 1.4rem;
        font-weight: 700;
        margin-bottom: 15px;
        border-left: 4px solid #667eea;
        padding-left: 10px;
    }
    
    /* æŒ‰é’®æ ·å¼ */
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white !important;
        border: none;
        border-radius: 8px;
        padding: 12px 30px;
        font-weight: 600;
        font-size: 1rem;
        transition: all 0.3s ease;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(102, 126, 234, 0.6);
    }
    
    /* ç‰¹å¾åˆ†æå¡ç‰‡ */
    .feature-card {
        background: linear-gradient(135deg, #f5f7fa 0%, #e9ecef 100%);
        border-radius: 10px;
        padding: 15px;
        margin: 10px 0;
        border-left: 4px solid #667eea;
    }
    
    .feature-name {
        color: #1e3c72;
        font-weight: 600;
        font-size: 0.95rem;
        margin-bottom: 5px;
    }
    
    .feature-value {
        color: #667eea;
        font-weight: 700;
        font-size: 1.2rem;
    }
    
    /* ç»“è®ºç›’å­ */
    .conclusion-box {
        border-radius: 10px;
        padding: 20px;
        margin: 20px 0;
        border-left: 5px solid #667eea;
    }
    
    .conclusion-ai {
        background: rgba(102, 126, 234, 0.1);
        border-left-color: #667eea;
    }
    
    .conclusion-human {
        background: rgba(245, 87, 108, 0.1);
        border-left-color: #f5576c;
    }
    
    .conclusion-mixed {
        background: rgba(255, 193, 7, 0.1);
        border-left-color: #ffc107;
    }
    
    /* è¿›åº¦æ¡å®¹å™¨ */
    .progress-container {
        background: white;
        border-radius: 12px;
        padding: 20px;
        margin: 20px 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
    }
    
    /* å“åº”å¼ */
    @media (max-width: 768px) {
        .main-title {
            font-size: 2rem;
        }
        .card-value {
            font-size: 2rem;
        }
    }
    </style>
    """, unsafe_allow_html=True)

load_custom_css()



# ============================================================
# å¥å­ä¸è¯å…ƒå¤„ç†
# ============================================================

def remove_emoji(text):
    emoji_pattern = re.compile(
    "["                 
    "\U0001F600-\U0001F64F"  # emoticons
    "\U0001F300-\U0001F5FF"  # symbols & pictographs
    "\U0001F680-\U0001F6FF"  # transport & map symbols
    "\U0001F1E0-\U0001F1FF"  # flags
    "\U00002700-\U000027BF"  # dingbats
    "\U0001F900-\U0001F9FF"  # supplemental symbols
    "\U0001FA70-\U0001FAFF"  # symbols extended-A
    "]+",
    flags=re.UNICODE)
    return emoji_pattern.sub("", text)

def split_sentences(text: str) -> list:
    """åˆ†å‰²å¥å­"""
    parts = re.split(r'[ã€‚ï¼ï¼Ÿ!?\n]+', text)
    return [p.strip() for p in parts if p.strip()]

def tokenize(text):
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—ï¼Œå…¶ä»–å…¨éƒ¨ç•¶ä½œ noise ä¸Ÿæ‰
    tokens = re.findall(r'[0-9A-Za-z]+|[\u4e00-\u9fa5]+', text)
    return tokens


# ============================================================
# ç‰¹å¾µæŠ½å–å™¨
# ============================================================

class AIDetectorFeatureExtractor:
    """AIæ–‡æœ¬ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.feature_names = [
            'sentence_length_mean',
            'sentence_length_std',
            'burstiness',
            'type_token_ratio',
            'avg_word_length',
            'punctuation_ratio',
            'function_word_ratio',
            'comma_ratio',
            'lexical_diversity',
            'entropy_word_freq',
            'zipf_tail_ratio',
            'repeated_structures',
            'common_connectors_ratio',
            'question_mark_ratio',
            'exclamation_ratio',
            'passive_voice_indicator',
            'avg_entropy_per_sentence',
        ]

    def extract_features(self, text: str) -> Dict[str, float]:
        """æå–ç‰¹å¾"""
        features = {}
        text = text.strip()

        if len(text) == 0:
            return {name: 0.0 for name in self.feature_names}

        sentences = split_sentences(text)
        sentence_lengths = [len(tokenize(s)) for s in sentences]

        features['sentence_length_mean'] = float(np.mean(sentence_lengths)) if sentence_lengths else 0.0
        features['sentence_length_std'] = float(np.std(sentence_lengths)) if sentence_lengths else 0.0

        if features['sentence_length_mean'] > 0:
            features['burstiness'] = features['sentence_length_std'] / features['sentence_length_mean']
        else:
            features['burstiness'] = 0.0

        words = tokenize(text.lower())
        unique_words = len(set(words)) if words else 0
        total_words = len(words)

        if total_words > 0:
            features['type_token_ratio'] = unique_words / total_words
            features['lexical_diversity'] = unique_words / total_words
        else:
            features['type_token_ratio'] = 0.0
            features['lexical_diversity'] = 0.0

        features['avg_word_length'] = float(np.mean([len(w) for w in words])) if words else 0.0

        total_chars = len(text)
        punct_count = sum(1 for c in text if not c.isalnum() and not c.isspace())
        features['punctuation_ratio'] = float(punct_count / total_chars) if total_chars else 0.0

        features['comma_ratio'] = float(text.count('ï¼Œ') / len(sentences)) if sentences else 0.0
        features['question_mark_ratio'] = float(text.count('ï¼Ÿ') / len(sentences)) if sentences else 0.0
        features['exclamation_ratio'] = float(text.count('ï¼') / len(sentences)) if sentences else 0.0

        function_words = ['çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'ä»¥', 'æœ‰', 'ç­‰', 'èˆ‡', 'æˆ–']
        fw_count = sum(text.count(fw) for fw in function_words)
        features['function_word_ratio'] = float(fw_count / total_words) if total_words else 0.0

        connectors = ['å› æ­¤', 'å¦å¤–', 'åŒæ™‚', 'ç¸½ä¹‹', 'é¦–å…ˆ']
        conn_count = sum(text.count(c) for c in connectors)
        features['common_connectors_ratio'] = float(conn_count / len(sentences)) if sentences else 0.0

        if total_words > 0:
            freq = Counter(words)
            rare = sum(1 for w, f in freq.items() if f == 1)
            features['zipf_tail_ratio'] = float(rare / len(freq))
        else:
            features['zipf_tail_ratio'] = 0.0

        features['entropy_word_freq'] = self._entropy(words)
        features['repeated_structures'] = 0.0
        features['passive_voice_indicator'] = float(text.count('è¢«') / len(sentences)) if sentences else 0.0
        features['avg_entropy_per_sentence'] = features['entropy_word_freq']

        return features

    def _entropy(self, words: list) -> float:
        """è®¡ç®—ç†µ"""
        if not words:
            return 0.0
        freq = Counter(words)
        total = len(words)
        entropy = 0.0
        for f in freq.values():
            p = f / total
            entropy -= p * np.log2(p)
        return float(entropy / np.log2(len(freq))) if freq else 0.0

# ============================================================
# å†…ç½®æ¨¡å‹
# ============================================================

class AIDetectorModel:
    """AIæ£€æµ‹æ¨¡å‹"""
    
    def __init__(self):
        self.extractor = AIDetectorFeatureExtractor()
        self.scaler = StandardScaler()
        self.model = LogisticRegression(max_iter=1000, random_state=42)
        self.rf = RandomForestClassifier(n_estimators=200, random_state=42)
        self.is_trained = False

    def train_sample_model(self):
        """è®­ç»ƒæ¨¡å‹"""
        n = 120
        features = []
        labels = []

        # AIæ ·æœ¬
        for _ in range(n // 2):
            sample = [
                np.random.normal(15, 3),
                np.random.normal(4, 1),
                np.random.normal(0.25, 0.05),
                np.random.normal(0.55, 0.05),
                np.random.normal(3.2, 0.3),
                np.random.normal(0.08, 0.02),
                np.random.normal(0.24, 0.05),
                np.random.normal(0.7, 0.15),
                np.random.normal(0.55, 0.05),
                np.random.normal(3.2, 0.4),
                np.random.normal(0.35, 0.08),
                np.random.normal(0.05, 0.03),
                np.random.normal(0.5, 0.1),
                np.random.normal(0.05, 0.02),
                np.random.normal(0.02, 0.01),
                np.random.normal(0.10, 0.03),
                np.random.normal(2.0, 0.3),
            ]
            features.append(sample)
            labels.append(1)

        # Humanæ ·æœ¬
        for _ in range(n // 2):
            sample = [
                np.random.normal(12, 5),
                np.random.normal(8, 2),
                np.random.normal(0.6, 0.15),
                np.random.normal(0.7, 0.1),
                np.random.normal(3.0, 0.5),
                np.random.normal(0.12, 0.03),
                np.random.normal(0.2, 0.05),
                np.random.normal(0.4, 0.2),
                np.random.normal(0.7, 0.1),
                np.random.normal(4.5, 0.4),
                np.random.normal(0.55, 0.1),
                np.random.normal(0.12, 0.05),
                np.random.normal(0.2, 0.05),
                np.random.normal(0.15, 0.05),
                np.random.normal(0.05, 0.02),
                np.random.normal(0.04, 0.02),
                np.random.normal(3.2, 0.5),
            ]
            features.append(sample)
            labels.append(0)

        X = np.array(features)
        y = np.array(labels)

        X_scaled = self.scaler.fit_transform(X)

        self.model.fit(X_scaled, y)
        self.rf.fit(X_scaled, y)
        self.is_trained = True

    def predict(self, text: str) -> Tuple[float, float, Dict]:
        """é¢„æµ‹"""
        text = remove_emoji(text)

        if not self.is_trained:
            self.train_sample_model()

        f = self.extractor.extract_features(text)
        X = np.array([f[name] for name in self.extractor.feature_names]).reshape(1, -1)
        X_scaled = self.scaler.transform(X)

        p1 = self.model.predict_proba(X_scaled)[0][1]
        p2 = self.rf.predict_proba(X_scaled)[0][1]
        ai_prob = (p1 + p2) / 2
        ai_prob = float(ai_prob)
        human_prob = 1 - ai_prob
        return ai_prob, human_prob, f


# ============================================================
# Streamlit UI - ç¾åŒ–ç‰ˆ
# ============================================================

@st.cache_resource
def init_model():
    """ç¼“å­˜æ¨¡å‹"""
    return AIDetectorModel()

def render_header():
    """æ¸²æŸ“æ ‡é¢˜"""
    st.markdown('<div class="main-title">ğŸ¤– AI vs Human æ–‡ç« åµæ¸¬å™¨</div>', unsafe_allow_html=True)
    st.markdown('<div class="subtitle">âœ¨ ä½¿ç”¨AIæŠ€è¡“ç²¾æº–åˆ¤æ–­æ–‡æœ¬æ¥æº âœ¨</div>', unsafe_allow_html=True)

def render_input_section() -> str:
    """æ¸²æŸ“è¾“å…¥åŒºåŸŸ"""
    st.markdown('<div class="section-title">ğŸ“ è¼¸å…¥æ–‡æœ¬</div>', unsafe_allow_html=True)
    
    text = st.text_area(
        "åœ¨ä¸‹æ–¹è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬ï¼ˆè‡³å°‘ 50 å­—ï¼‰:",
        height=220,
        placeholder="ç²˜è´´ä½ çš„æ–‡æœ¬å†…å®¹... æ”¯æŒä¸­è‹±æ–‡æ··åˆ"
    )
    
    return text

def render_result_cards(ai_prob: float, human_prob: float):
    """æ¸²æŸ“ç»“æœå¡ç‰‡"""
    st.markdown('<div class="section-title">ğŸ“Š åˆ†æçµæœ</div>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        ai_label = "æå¯èƒ½ä¸º AI ç”Ÿæˆ" if ai_prob > 0.7 else "å¯èƒ½ä¸º AI ç”Ÿæˆ" if ai_prob > 0.5 else "å¯èƒ½ä¸ºäººç±»æ’°å†™"
        st.markdown(f"""
        <div class="result-card card-ai">
            <div class="card-title">ğŸ¤– AI ç”Ÿæˆæ¦‚ç‡</div>
            <div class="card-value">{ai_prob*100:.1f}%</div>
            <div class="card-label">{ai_label}</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        human_label = "æå¯èƒ½ä¸ºäººç±»æ’°å†™" if human_prob > 0.7 else "å¯èƒ½ä¸ºäººç±»æ’°å†™" if human_prob > 0.5 else "å¯èƒ½ä¸º AI ç”Ÿæˆ"
        st.markdown(f"""
        <div class="result-card card-human">
            <div class="card-title">ğŸ‘¤ äººé¡æ’°å¯«æ¦‚ç‡</div>
            <div class="card-value">{human_prob*100:.1f}%</div>
            <div class="card-label">{human_label}</div>
        </div>
        """, unsafe_allow_html=True)

def render_progress_bar(ai_prob: float):
    """æ¸²æŸ“è¿›åº¦æ¡"""
    st.markdown('<div class="progress-container">', unsafe_allow_html=True)
    
    col_label1, col_label2, col_label3 = st.columns([1, 1, 1])
    with col_label1:
        st.caption("ğŸ¤– AI")
    with col_label3:
        st.caption("ğŸ‘¤ Human")
    
    progress_html = f"""
    <div style="display: flex; margin: 20px 0; border-radius: 10px; overflow: hidden; background: #e9ecef;">
        <div style="width: {ai_prob*100:.1f}%; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); height: 40px; display: flex; align-items: center; justify-content: center; color: white; font-weight: bold; font-size: 0.9rem;">
            {ai_prob*100:.1f}%
        </div>
        <div style="width: {(1-ai_prob)*100:.1f}%; background: linear-gradient(90deg, #f093fb 0%, #f5576c 100%); height: 40px; display: flex; align-items: center; justify-content: center; color: white; font-weight: bold; font-size: 0.9rem;">
            {(1-ai_prob)*100:.1f}%
        </div>
    </div>
    """
    st.markdown(progress_html, unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)

def render_features(features_dict: Dict):
    """æ¸²æŸ“ç‰¹å¾åˆ†æ"""
    if st.checkbox("ğŸ“‹ æ˜¾ç¤ºè©³ç´°ç‰¹å¾µåˆ†æ", value=False):
        st.markdown('<div class="section-title">ğŸ”¬ ç‰¹å¾è¯¦æƒ…</div>', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**å¥å­èŠ‚å¥ç‰¹å¾**")
            for name in ['sentence_length_mean', 'sentence_length_std', 'burstiness']:
                st.markdown(f"""
                <div class="feature-card">
                    <div class="feature-name">{name}</div>
                    <div class="feature-value">{features_dict[name]:.3f}</div>
                </div>
                """, unsafe_allow_html=True)
        
        with col2:
            st.markdown("**è¯æ±‡ç‰¹å¾**")
            for name in ['type_token_ratio', 'avg_word_length', 'entropy_word_freq']:
                st.markdown(f"""
                <div class="feature-card">
                    <div class="feature-name">{name}</div>
                    <div class="feature-value">{features_dict[name]:.3f}</div>
                </div>
                """, unsafe_allow_html=True)

def render_visualization(text: str):
    """æ¸²æŸ“å›¾è¡¨"""
    if st.checkbox("ğŸ“ˆ æ˜¾ç¤ºå¯è¦–åŒ–åœ–è¡¨", value=False):
        st.markdown('<div class="section-title">ğŸ“Š æ•°æ®å¯è§†åŒ–</div>', unsafe_allow_html=True)
        
        sentences = split_sentences(text)
        sentence_lengths = [len(tokenize(s)) for s in sentences]
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig, ax = plt.subplots(figsize=(8, 5))
            ax.hist(sentence_lengths, bins=max(5, len(set(sentence_lengths))),
                    color='#667eea', alpha=0.7, edgecolor='#764ba2', linewidth=2)
            ax.set_xlabel('Sentence Length (tokens)', fontweight='bold')
            ax.set_ylabel('Frequency', fontweight='bold')
            ax.set_title('å¥é•·åˆ†ä½ˆ', fontweight='bold', fontsize=12)
            ax.grid(alpha=0.3)
            plt.tight_layout()
            st.pyplot(fig, use_container_width=True)
        
        with col2:
            words = tokenize(text.lower())
            if words:
                freq = Counter(words)
                top_words = dict(freq.most_common(10))
                
                fig, ax = plt.subplots(figsize=(8, 5))
                ax.barh(list(top_words.keys()), list(top_words.values()), 
                       color='#f5576c', alpha=0.7, edgecolor='#764ba2', linewidth=2)
                ax.set_xlabel('Frequency', fontweight='bold')
                ax.set_title('é«˜é »è©å½™ (Top 10)', fontweight='bold', fontsize=12)
                ax.invert_yaxis()
                plt.tight_layout()
                st.pyplot(fig, use_container_width=True)

def render_conclusion(ai_prob: float):
    """æ¸²æŸ“ç»“è®º"""
    st.markdown('<div class="section-title">ğŸ¯ åˆ¤å®šçµè«–</div>', unsafe_allow_html=True)
    
    if ai_prob > 0.7:
        st.markdown("""
        <div class="conclusion-box conclusion-ai">
            <h4>âš ï¸ æ¥µå¯èƒ½ç‚º AI ç”Ÿæˆ</h4>
            <p>è©²æ–‡æœ¬å‘ˆç¾å‡ºä»¥ä¸‹å…¸å‹çš„ AI ç‰¹å¾µ:</p>
            <ul>
                <li>âœ“ å¥å­ç¯€å¥å¹³ç©© (ä½ Burstiness)</li>
                <li>âœ“ å¸¸è¦‹é€£æ¥è©ä½¿ç”¨è¼ƒé »ç¹</li>
                <li>âœ“ è©å½™åˆ†å¸ƒè¼ƒè¦å‰‡å’Œå‡å‹»</li>
                <li>âœ“ æ–‡é¢¨é«˜åº¦ä¸€è‡´</li>
            </ul>
            <p><strong>å»ºè­°:</strong> éœ€è¦é€²ä¸€æ­¥äººå·¥å¯©æŸ¥ç¢ºèª</p>
        </div>
        """, unsafe_allow_html=True)
    elif ai_prob > 0.5:
        st.markdown("""
        <div class="conclusion-box conclusion-mixed">
            <h4>âš¡ æ··åˆç‰¹å¾µ - å¯èƒ½ç‚º AI ç”Ÿæˆæˆ–ç¶“éå¤§å¹…ç·¨è¼¯</h4>
            <p>è©²æ–‡æœ¬å±•ç¾äº†æ··åˆç‰¹å¾µï¼Œé›£ä»¥ç¢ºå®šä¾†æº:</p>
            <ul>
                <li>~ éƒ¨åˆ†ç‰¹å¾µèˆ‡ AI ç›¸ç¬¦</li>
                <li>~ éƒ¨åˆ†ç‰¹å¾µèˆ‡äººé¡ç›¸ç¬¦</li>
                <li>~ å¯èƒ½æ˜¯äººé¡ç·¨è¼¯çš„ AI å…§å®¹ï¼Œæˆ– AI æ½¤è‰²çš„äººé¡æ–‡æœ¬</li>
            </ul>
            <p><strong>å»ºè­°:</strong> å»ºè­°çµåˆäººå·¥å¯©æŸ¥å’Œå…¶ä»–æ–¹æ³•åˆ¤æ–·</p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown("""
        <div class="conclusion-box conclusion-human">
            <h4>âœ… æ¥µå¯èƒ½ç‚ºäººé¡æ’°å¯«</h4>
            <p>è©²æ–‡æœ¬å‘ˆç¾å‡ºä»¥ä¸‹å…¸å‹çš„äººé¡ç‰¹å¾µ:</p>
            <ul>
                <li>âœ“ å¥å­é•·åº¦æ³¢å‹•è¼ƒå¤§ (é«˜ Burstiness)</li>
                <li>âœ“ è©å½™é¸æ“‡å¤šæ¨£æ€§é«˜</li>
                <li>âœ“ å­˜åœ¨è‡ªç„¶çš„èªè¨€ä¸è¦å‰‡æ€§</li>
                <li>âœ“ å€‹äººé¢¨æ ¼æ˜é¡¯</li>
            </ul>
            <p><strong>è©•ä¼°:</strong> è©²æ–‡æœ¬å¾ˆå¯èƒ½å‡ºè‡ªçœŸäººæ‰‹ç­†</p>
        </div>
        """, unsafe_allow_html=True)

def main():
    """ä¸»å‡½æ•°"""
    render_header()
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.markdown("### âš™ï¸ è¨­å®šèˆ‡èªªæ˜")
        
        st.info("""
        **ğŸ“– ä½¿ç”¨æ–¹å¼**
        1. åœ¨æ–‡æœ¬æ¡†ä¸­è¼¸å…¥è¦åˆ†æçš„æ–‡æœ¬
        2. é»æ“Šã€ŒğŸ” ç«‹å³åˆ†æã€æŒ‰éˆ•
        3. æŸ¥çœ‹ AI æ¦‚ç‡å’Œè©³ç´°åˆ†æ
        
        **ğŸ”¬ åµæ¸¬åŸç†**
        - Burstiness: å¥å­ç¯€å¥åˆ†æ
        - TTR: è©å½™å¤šæ¨£æ€§
        - Entropy: è©é »ç†µè¨ˆç®—
        - Stylometry: æ–‡é¢¨çµ±è¨ˆ
        - Zipf's Law: é•·å°¾è©åˆ†æ
        
        **ğŸ’¡ æ³¨æ„äº‹é …**
        - æœ¬å·¥å…·åŸºæ–¼çµ±è¨ˆç‰¹å¾µ
        - ä¸èƒ½ä½œç‚ºå”¯ä¸€åˆ¤æ–·ä¾æ“š
        - çµæœæº–ç¢ºåº¦å—æ–‡æœ¬é•·åº¦å½±éŸ¿
        """)
        
        st.divider()
        st.caption("ğŸš€ ç”± OpenSpec é©…å‹• | v1.0")
    
    # ä¸»åŒºåŸŸ
    text = render_input_section()
    
    # åˆ†ææŒ‰é’®
    col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 2])
    
    with col_btn1:
        analyze_btn = st.button("ğŸ” ç«‹å³åˆ†æ", use_container_width=True)
    
    # æ‰§è¡Œåˆ†æ
    if analyze_btn:
        if len(text.strip()) < 50:
            st.warning("âš ï¸ è«‹è¼¸å…¥è‡³å°‘ 50 å€‹å­—çš„æ–‡æœ¬ä¾†åˆ†æ", icon="ğŸ“")
            return
        
        try:
            with st.spinner("ğŸ”„ æ­£åœ¨åˆ†ææ–‡æœ¬..."):
                model = init_model()
                ai_prob, human_prob, features_dict = model.predict(text)
            
            # ä¿å­˜ç»“æœåˆ° session
            st.session_state.last_result = {
                'ai_prob': ai_prob,
                'human_prob': human_prob,
                'features': features_dict,
                'text': text
            }
            
            st.success("âœ… åˆ†æå®Œæˆ!", icon="ğŸ‰")
            
        except Exception as e:
            st.error(f"âŒ åˆ†æå¤±æ•—: {str(e)}", icon="ğŸ’¥")
    
    # æ˜¾ç¤ºç»“æœ
    if 'last_result' in st.session_state:
        result = st.session_state.last_result
        
        render_result_cards(result['ai_prob'], result['human_prob'])
        render_progress_bar(result['ai_prob'])
        render_features(result['features'])
        render_visualization(result['text'])
        render_conclusion(result['ai_prob'])
        
        st.divider()
        st.caption("ğŸ’» Powered by Streamlit | Made with â¤ï¸")

if __name__ == "__main__":
    main()
